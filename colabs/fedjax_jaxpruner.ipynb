{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "d-jMJyS0S3ya"
      },
      "outputs": [],
      "source": [
        "#@title LICENSE\n",
        "# Licensed under the Apache License, Version 2.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XnDKEn_TTMO7"
      },
      "source": [
        "# JaxPruner X FedJAX\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/google-research/jaxpruner/blob/main/colabs/fedjax_jaxpruner.ipynb)\n",
        "\n",
        "This interactive colab explores pruning in federated learning. It demonstrates how to easily experiment with various pruning methods on federated training of EMNIST using FedJAX with the `jaxpruner` library.\n",
        "\n",
        "Pruning is conducted on the server, and is easily added to federated training by\n",
        "* wrapping the existing server optax optimizer with a `jaxpruner` updater\n",
        "* applying a post gradient update step on the server model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xkBZ3xl5JMck"
      },
      "source": [
        "### Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DO792Gi0WByi"
      },
      "outputs": [],
      "source": [
        "import collections\n",
        "from ml_collections import config_dict\n",
        "from typing import Any, Callable, Mapping, Sequence, Tuple\n",
        "\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import optax\n",
        "\n",
        "import fedjax\n",
        "\n",
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fHfPZw6l2BUZ"
      },
      "outputs": [],
      "source": [
        "!pip3 install git+https://github.com/google-research/jaxpruner\n",
        "import jaxpruner"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-l5rlD8zJMcn"
      },
      "source": [
        "### Configure Pruning Algorithm\n",
        "\n",
        "Here we specify the sparsity config to define the pruning algorithm to be used."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yH6Fu4tsZBf-"
      },
      "outputs": [],
      "source": [
        "SPARSITY_CONFIG = config_dict.ConfigDict()\n",
        "SPARSITY_CONFIG.algorithm = 'magnitude' # what pruning algorithm to apply, options: 'magnitude', 'random', 'saliency'\n",
        "SPARSITY_CONFIG.dist_type = 'uniform' # how to distribute sparsity to variables, options:  str, 'erk', 'uniform'\n",
        "SPARSITY_CONFIG.update_freq = 10 # how often to apply pruning\n",
        "SPARSITY_CONFIG.update_start_step = 250 # when to start pruning\n",
        "SPARSITY_CONFIG.update_end_step = 750 # when to stop pruning\n",
        "SPARSITY_CONFIG.sparsity = 0.8 # target sparsity"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i6-B7x22JMcn"
      },
      "source": [
        "Let's also write a utility to keep track of sparsity metrics."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g1kIT_K1oHmW"
      },
      "outputs": [],
      "source": [
        "def get_sparsity_metrics(params):\n",
        "  sparsity_summary_dict = jaxpruner.utils.summarize_sparsity(\n",
        "      param_tree=params, only_total_sparsity=False\n",
        "  )\n",
        "  return collections.OrderedDict(sparsity_summary_dict)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oUU9IA1KJMco"
      },
      "source": [
        "We create a `jaxpruner` updater from the given sparsity config. This is used to update the server optimization step to include pruning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2KX0mXg_Yy5h"
      },
      "outputs": [],
      "source": [
        "server_updater = jaxpruner.create_updater_from_config(SPARSITY_CONFIG)\n",
        "server_gradient_transform = optax.adam(learning_rate=10**(-2.5), b1=0.9, b2=0.999, eps=10**(-4))\n",
        "server_gradient_transform = server_updater.wrap_optax(server_gradient_transform)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_0mylFvSGLpY"
      },
      "source": [
        "### FedJAX Federated Averaging Implementation\n",
        "\n",
        "This federated averaging implementation is forked from https://github.com/google/fedjax/blob/main/fedjax/algorithms/fed_avg.py, with minor changes to just two lines of code to incorporate `jaxpruner`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4ao1BULVW3LZ"
      },
      "outputs": [],
      "source": [
        "ClientId = bytes\n",
        "Grads = fedjax.Params\n",
        "\n",
        "\n",
        "@fedjax.dataclass\n",
        "class ServerState:\n",
        "  \"\"\"State of server passed between rounds.\n",
        "  Attributes:\n",
        "    params: A pytree representing the server model parameters.\n",
        "    opt_state: A pytree representing the server optimizer state.\n",
        "  \"\"\"\n",
        "  params: fedjax.Params\n",
        "  opt_state: fedjax.OptState\n",
        "\n",
        "\n",
        "def federated_averaging(\n",
        "    grad_fn: Callable[[fedjax.Params, fedjax.BatchExample, fedjax.PRNGKey],\n",
        "                      Grads],\n",
        "    client_optimizer: fedjax.Optimizer,\n",
        "    server_optimizer: fedjax.Optimizer,\n",
        "    server_updater: jaxpruner.BaseUpdater,  # CHANGE #1: include the jaxpruner updater here!\n",
        "    client_batch_hparams: fedjax.ShuffleRepeatBatchHParams\n",
        ") -\u003e fedjax.FederatedAlgorithm:\n",
        "  \"\"\"Builds the basic implementation of federated averaging.\"\"\"\n",
        "\n",
        "  def init(params: fedjax.Params) -\u003e ServerState:\n",
        "    opt_state = server_optimizer.init(params)\n",
        "    return ServerState(params, opt_state)\n",
        "\n",
        "  def apply(\n",
        "      server_state: ServerState,\n",
        "      clients: Sequence[Tuple[ClientId, fedjax.ClientDataset, fedjax.PRNGKey]]\n",
        "  ) -\u003e Tuple[ServerState, Mapping[ClientId, Any]]:\n",
        "    client_diagnostics = {}\n",
        "    # We use a list here for clarity, but we strongly recommend avoiding loading\n",
        "    # all client outputs into memory since the outputs can be quite large\n",
        "    # depending on the size of the model.\n",
        "    client_delta_params_weights = []\n",
        "    for client_id, client_dataset, client_rng in clients:\n",
        "      delta_params = client_update(server_state.params, client_dataset,\n",
        "                                   client_rng)\n",
        "      client_delta_params_weights.append((delta_params, len(client_dataset)))\n",
        "      # We record the l2 norm of client updates as an example, but it is not\n",
        "      # required for the algorithm.\n",
        "      client_diagnostics[client_id] = {\n",
        "          'delta_l2_norm': fedjax.tree_util.tree_l2_norm(delta_params)\n",
        "      }\n",
        "    mean_delta_params = fedjax.tree_util.tree_mean(client_delta_params_weights)\n",
        "    server_state = server_update(server_state, mean_delta_params)\n",
        "    return server_state, client_diagnostics\n",
        "\n",
        "  def client_update(server_params, client_dataset, client_rng):\n",
        "    params = server_params\n",
        "    opt_state = client_optimizer.init(params)\n",
        "    for batch in client_dataset.shuffle_repeat_batch(client_batch_hparams):\n",
        "      client_rng, use_rng = jax.random.split(client_rng)\n",
        "      grads = grad_fn(params, batch, use_rng)  # pytype: disable=wrong-arg-types  # jax-ndarray\n",
        "      opt_state, params = client_optimizer.apply(grads, opt_state, params)\n",
        "    delta_params = jax.tree_util.tree_map(lambda a, b: a - b,\n",
        "                                               server_params, params)\n",
        "    return delta_params\n",
        "\n",
        "  def server_update(server_state, mean_delta_params):\n",
        "    opt_state, params = server_optimizer.apply(mean_delta_params,\n",
        "                                               server_state.opt_state,\n",
        "                                               server_state.params)\n",
        "    # CHANGE #2: apply the jaxpruner updater's post gradient update procedure\n",
        "    # to the server model.\n",
        "    updated_params = server_updater.post_gradient_update(params, opt_state)\n",
        "    return ServerState(updated_params, opt_state)\n",
        "\n",
        "  return fedjax.FederatedAlgorithm(init, apply)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yEjoFBzfJMcp"
      },
      "source": [
        "### Federated Training\n",
        "\n",
        "\n",
        "We are training a CNN model for character recognition on EMNIST, as in https://github.com/google/fedjax/blob/main/examples/emnist_fed_avg.py."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9JM1zvN8JMcq"
      },
      "outputs": [],
      "source": [
        "# Load train and test federated data for EMNIST.\n",
        "train_fd, test_fd = fedjax.datasets.emnist.load_data(only_digits=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t9xmeyfvJMcq"
      },
      "outputs": [],
      "source": [
        "# Create CNN model with dropout.\n",
        "model = fedjax.models.emnist.create_conv_model(only_digits=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zuI82LMPJMcq"
      },
      "outputs": [],
      "source": [
        "# Scalar loss function with model parameters, batch of examples, and seed\n",
        "# PRNGKey as input.\n",
        "def loss(params, batch, rng):\n",
        "  # `rng` used with `apply_for_train` to apply dropout during training.\n",
        "  preds = model.apply_for_train(params, batch, rng)\n",
        "  # Per example loss of shape [batch_size].\n",
        "  example_loss = model.train_loss(batch, preds)\n",
        "  return jnp.mean(example_loss)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LQKpyfF4JMcq"
      },
      "outputs": [],
      "source": [
        "# Gradient function of `loss` w.r.t. to model `params` (jitted for speed).\n",
        "grad_fn = jax.jit(jax.grad(loss))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yetlxpSlJMcp"
      },
      "source": [
        "When instantiating the server optimizer, use the optax gradient transform produced by the `jaxpruner` updater that includes the pruning procedure."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A8ne6yMZYrEw"
      },
      "outputs": [],
      "source": [
        "# Create federated averaging algorithm, with server-side pruning.\n",
        "client_optimizer = fedjax.optimizers.sgd(learning_rate=10**(-1.5))\n",
        "server_optimizer = fedjax.optimizers.create_optimizer_from_optax(\n",
        "    server_gradient_transform  # note this includes the pruning algorithm\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O2ksY0IOJMcp"
      },
      "source": [
        "Our federated averagining procedure, given above, applies a post gradient update pruning procedure to the server model, and requires the server updater as an argument."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x8r-CmscYxPC"
      },
      "outputs": [],
      "source": [
        "# Hyperparameters for client local traing dataset preparation.\n",
        "client_batch_hparams = fedjax.ShuffleRepeatBatchHParams(batch_size=20)\n",
        "algorithm = federated_averaging(grad_fn, client_optimizer,\n",
        "                                server_optimizer,\n",
        "                                server_updater,  # note jaxpruner updater is a required arg\n",
        "                                client_batch_hparams)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZRldsz6zJMcq"
      },
      "source": [
        "With pruning  integrated into the server optimizer and federated averaging algorithm, we can conduct federated training as usual."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DBUnOHkUa94X"
      },
      "outputs": [],
      "source": [
        "# Initialize model parameters and algorithm server state.\n",
        "init_params = model.init(jax.random.PRNGKey(17))\n",
        "server_state = algorithm.init(init_params)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VyXbiylpbAxm"
      },
      "outputs": [],
      "source": [
        "training_metrics_log = []\n",
        "test_metrics_log = []\n",
        "sparsity_metrics_log = []\n",
        "\n",
        "# Train and eval loop.\n",
        "train_client_sampler = fedjax.client_samplers.UniformGetClientSampler(\n",
        "    fd=train_fd, num_clients=10, seed=0)\n",
        "for round_num in range(1, 1501):\n",
        "  # Sample 10 clients per round without replacement for training.\n",
        "  clients = train_client_sampler.sample()\n",
        "  # Run one round of training on sampled clients.\n",
        "  server_state, client_diagnostics = algorithm.apply(server_state, clients)\n",
        "  print(f'[round {round_num}]')\n",
        "  # Optionally print client diagnostics if curious about each client's model\n",
        "  # update's l2 norm.\n",
        "  # print(f'[round {round_num}] client_diagnostics={client_diagnostics}')\n",
        "\n",
        "  if round_num % 10 == 0:\n",
        "    # Periodically evaluate the trained server model parameters.\n",
        "    # Read and combine clients' train and test datasets for evaluation.\n",
        "    client_ids = [cid for cid, _, _ in clients]\n",
        "    train_eval_datasets = [cds for _, cds in train_fd.get_clients(client_ids)]\n",
        "    test_eval_datasets = [cds for _, cds in test_fd.get_clients(client_ids)]\n",
        "    train_eval_batches = fedjax.padded_batch_client_datasets(\n",
        "        train_eval_datasets, batch_size=256)\n",
        "    test_eval_batches = fedjax.padded_batch_client_datasets(\n",
        "        test_eval_datasets, batch_size=256)\n",
        "\n",
        "    # Run evaluation metrics defined in `model.eval_metrics`.\n",
        "    train_metrics = fedjax.evaluate_model(model, server_state.params,  # pytype: disable=wrong-arg-types  # jax-ndarray\n",
        "                                          train_eval_batches)\n",
        "    training_metrics_log.append(train_metrics)\n",
        "    test_metrics = fedjax.evaluate_model(model, server_state.params,  # pytype: disable=wrong-arg-types  # jax-ndarray\n",
        "                                          test_eval_batches)\n",
        "    sparsity_metrics = get_sparsity_metrics(server_state.params)\n",
        "    sparsity_metrics_log.append(sparsity_metrics)\n",
        "    test_metrics_log.append(test_metrics)\n",
        "    print(f'[round {round_num}] train_metrics={train_metrics}')\n",
        "    print(f'[round {round_num}] test_metrics={test_metrics}')\n",
        "    print(f'[round {round_num}] sparsity_metrics={sparsity_metrics}')\n",
        "\n",
        "# Save final trained model parameters to file.\n",
        "fedjax.serialization.save_state(server_state.params, '/tmp/params')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qeB75379Wgjt"
      },
      "source": [
        "##  "
      ]
    }
  ],
  "metadata": {
    "colab": {
      "last_runtime": {
        "build_target": "//learning/grp/tools/ml_python:ml_notebook",
        "kind": "private"
      },
      "private_outputs": true,
      "provenance": [
        {
          "file_id": "1CWFyySAywJWYPodmbfpnxz7FOEKq0DjD",
          "timestamp": 1680589453840
        }
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
